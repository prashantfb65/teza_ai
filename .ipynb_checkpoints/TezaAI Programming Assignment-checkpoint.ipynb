{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TezaAI programming assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook, following steps are needed:\n",
    "\n",
    "Requirement:\n",
    "1. Python 3.6.5\n",
    "\n",
    "Open Terminal\n",
    "\n",
    "1. Navigate to the project folder (cd {downloaded_file}/teza_ai) \n",
    "2. Activate the virtual environment (. ./setup-env.sh)\n",
    "3. Install the dependepencies (pip install -r requirements.txt)\n",
    "4. Start \"Jupyter Notebook\" (jupyter notebook)\n",
    "5. Click on the notebook \"TezaAI Programming Assignment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries import\n",
    "\n",
    "import cerberus\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "from collections import Counter \n",
    "from pprint import pprint\n",
    "from deepmerge import always_merger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Files Validation schema\n",
    "\n",
    "This peace of code below validates the input files format.\n",
    "i.e. if they are valid files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building validation dict based on the structure below\n",
    "\n",
    "# {\n",
    "#  \"_id\": \"https://www.ekmhinnovators.com/ekmh-innovators-blog-beta/interview-ourcrowd-ceo-jon-medved-on-impact-investing-crowdfunding\",\n",
    "#  \"title\": \"Interview: OurCrowd CEO Jon Medved on Crowdfunding, Beyond ...\",\n",
    "#  \"body\": \"EKMH Innovators Interview Series An interview ...\",\n",
    "#  \"origin\": \"google custom search\",\n",
    "#  \"feedId\": 103,\n",
    "#  \"jobId\": \"37b3e04c-cf7d-4032-82ad-a2bd89dc90ac\",\n",
    "#  \"person\": {\n",
    "# \t \"id\": \"16\",\n",
    "# \t \"name\": \"Jon Medved\"\n",
    "#  }\n",
    "# }\n",
    "\n",
    "STRING_MANDATORY = {'type': 'string', 'empty': False, 'required': True}\n",
    "INT_MANDATORY = {'type': 'integer', 'empty': False, 'required': True}\n",
    "VALIDATION_SCHEMA = {\n",
    "    '_id': STRING_MANDATORY, \n",
    "    'title': STRING_MANDATORY, \n",
    "    'body': STRING_MANDATORY, \n",
    "    'origin': STRING_MANDATORY, \n",
    "    'feedId': INT_MANDATORY, \n",
    "    'jobId': STRING_MANDATORY, \n",
    "    'person': {\n",
    "        'type': 'dict', 'required': True, 'empty': False, \n",
    "            'schema': {\n",
    "                'id': STRING_MANDATORY,\n",
    "                'name': STRING_MANDATORY\n",
    "            }\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': {'empty': False, 'required': True, 'type': 'string'},\n",
      " 'body': {'empty': False, 'required': True, 'type': 'string'},\n",
      " 'feedId': {'empty': False, 'required': True, 'type': 'integer'},\n",
      " 'jobId': {'empty': False, 'required': True, 'type': 'string'},\n",
      " 'origin': {'empty': False, 'required': True, 'type': 'string'},\n",
      " 'person': {'empty': False,\n",
      "            'required': True,\n",
      "            'schema': {'id': {'empty': False,\n",
      "                              'required': True,\n",
      "                              'type': 'string'},\n",
      "                       'name': {'empty': False,\n",
      "                                'required': True,\n",
      "                                'type': 'string'}},\n",
      "            'type': 'dict'},\n",
      " 'title': {'empty': False, 'required': True, 'type': 'string'}}\n"
     ]
    }
   ],
   "source": [
    "# displaying validation dictionary\n",
    "\n",
    "pprint(VALIDATION_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data global var\n",
    "\n",
    "DATA = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "validate input data schema\n",
    "\"\"\"\n",
    "def validate_schema(entities, schema, filepath):\n",
    "    checker = cerberus.Validator()\n",
    "    checker.allow_unknown = True\n",
    "    if checker.validate(entities, schema):\n",
    "        print(f'Input schema validated for {filepath} !')\n",
    "    else:\n",
    "        errors = checker.errors\n",
    "        proceed = False\n",
    "        error = errors\n",
    "        raise ValueError(f'Format mismatch for the input {errors}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Validate and read the JSON data\n",
    "file_path: path of the json file\n",
    "\"\"\"\n",
    "def read_and_validate_data(filepath):\n",
    "    with open(filepath) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        validate_schema(entities=data, schema=VALIDATION_SCHEMA,\\\n",
    "                        filepath=filepath)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before executing the block of code below, more files (interview JSONs) can be added to the data folder\n",
    "\n",
    "The data folder with the source code comprises of the already provided 6 JSON input interview files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/0.json', './data/1.json', './data/2.json', './data/3.json', './data/4.json', './data/5.json']\n"
     ]
    }
   ],
   "source": [
    "# Path of the data files\n",
    "# Add more files to the project folder or change this path to read from any \n",
    "# other directory path\n",
    "DATA_FOLDER = os.path.join(os.path.curdir, 'data') # by default it takes the JSONs from project folder\n",
    "\n",
    "# only considering JSON files at the moment\n",
    "interview_files_list = [os.path.join(DATA_FOLDER, f) for f in os.listdir(DATA_FOLDER) \\\n",
    "                        if '.json' in os.path.splitext(f) ]\n",
    "\n",
    "\n",
    "print(interview_files_list)\n",
    "if len(interview_files_list) < 1:\n",
    "    proceed = False\n",
    "    error = \"No data file available\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input schema validated for ./data/0.json !\n",
      "Input schema validated for ./data/1.json !\n",
      "Input schema validated for ./data/2.json !\n",
      "Input schema validated for ./data/3.json !\n",
      "Input schema validated for ./data/4.json !\n",
      "Input schema validated for ./data/5.json !\n"
     ]
    }
   ],
   "source": [
    "# Reading in the data input files while also \n",
    "# validating them\n",
    "DATA = [read_and_validate_data(file) for file in interview_files_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common programming methods\n",
    "\n",
    "Let us define some common methods before progressing with interview quotes extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Remove special character except the exceptions list\n",
    "\"\"\"\n",
    "def remove_special_character(text_to_process, exceptions=[], strip=False):\n",
    "    new_string = str(text_to_process)\n",
    "    if strip:\n",
    "        new_string = new_string.strip()\n",
    "    special_chars = string.printable[62:]\n",
    "    special_chars = [x for x in special_chars if x not in exceptions]\n",
    "    for char in text_to_process:\n",
    "        if char in special_chars:\n",
    "            new_string = new_string.replace(char, '')\n",
    "    return new_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Remove special character without spaces\n",
    "\"\"\"\n",
    "def remove_special_character_without_spaces(text_to_process, exceptions=[]):\n",
    "    return remove_special_character(text_to_process, [' '], strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Remove special character with custom\n",
    "\"\"\"\n",
    "def remove_special_character_with_custom(text_to_process, custom=[]):\n",
    "    return remove_special_character(text_to_process, custom, strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Remove spaces\n",
    "\"\"\"\n",
    "def remove_spaces(text_to_process):\n",
    "    return text_to_process.replace(\" \", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reverse the array\n",
    "\"\"\"\n",
    "def reverse_the_array(arr):\n",
    "    return arr[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom variables and methods for finding Interviewee and Interviewers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGULAR_EXPRESSION = '(?<=\\{})(.*?)(?=\\:)' # to match the names with article\n",
    "\n",
    "\n",
    "# article generally ends with a fullstop, question mark or semicolon..\n",
    "# add more to list if required\n",
    "INTERVIEW_ENDING = ['.', '?']  \n",
    "\n",
    "\n",
    "# Name threshold\n",
    "THRESHOLD = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Predicts the interviewee names based on the information\n",
    "provided in the json metadata while mapping it with the \n",
    "data\n",
    "\n",
    "\n",
    "Assumption: assuming that there is only interviewee\n",
    "\"\"\"\n",
    "\n",
    "def predict(name_doc_mapping={}, softening=False):\n",
    "    predicted_interviewee = {}\n",
    "    for key, value in name_doc_mapping.items():\n",
    "        item_list = max(value.items(), key=lambda x: x[1])\n",
    "        labels = list()\n",
    "        for k, v in value.items():\n",
    "            if v == item_list[1]:\n",
    "                predicted_interviewee[key] = k\n",
    "    return predicted_interviewee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Getting occurence of name in the interview text\n",
    "\"\"\"\n",
    "\n",
    "def element_occurence(name, interview_text):\n",
    "    return len([pos.start() for pos in re.finditer(f'{name}:', interview_text)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In cell interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1: Interviewee name based on name provided in JSON\n",
    "# And using that info evaluate what form of name is used\n",
    "# e.g. for a name Tom Hilton, in the article only \"Tom\" could be used\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# in-cell compilation of logic ..... Fetch names based \n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "doc = {}\n",
    "\n",
    "for idx, entity in enumerate(DATA):\n",
    "    \n",
    "    name_dict = {}\n",
    "    \n",
    "    interview_text = str.lower(entity.get('body')) # interview content\n",
    "    interviewee = entity.get('person')['name'] # person being interviewed\n",
    "    \n",
    "    sub_names = interviewee.split()\n",
    "    sub_names.append(interviewee)\n",
    "    names = [x.lower() for x in sub_names] # name list because only part of the name could be used\n",
    "    \n",
    "    \n",
    "    # create of a dictionary to map interviewer names used in the article\n",
    "    for name in names:\n",
    "        name_dict[name] = element_occurence(name, interview_text)\n",
    "    doc[f'doc_{idx}'] = name_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc_0': {'joe': 23, 'lonsdale': 0, 'joe lonsdale': 0},\n",
       " 'doc_1': {'balaji': 0, 'srinivasan': 13, 'balaji srinivasan': 0},\n",
       " 'doc_2': {'jon': 0, 'medved': 11, 'jon medved': 11},\n",
       " 'doc_3': {'phil': 0, 'libin': 64, 'phil libin': 64},\n",
       " 'doc_4': {'john': 0, 'mackey': 22, 'john mackey': 1},\n",
       " 'doc_5': {'tim': 0, 'cook': 43, 'tim cook': 43}}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 2: Interviewee & Interviewers name based on the article contents \n",
    "# and evaluating the results with the approach 1 result to find consolidated results\n",
    "# ------------------------------------------------------------------\n",
    "# in-cell compilation of logic ..... Fetch article based entities\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "doc_exprsn = {}\n",
    "\n",
    "for idx, entity in enumerate(DATA):\n",
    "    \n",
    "    count_dict = {}\n",
    "    all_matches = []\n",
    "    \n",
    "    interview_text = str.lower(entity.get('body')) # interview content\n",
    "    \n",
    "    # Now we will try to create the same kind of map for the interviewers & interviewees based\n",
    "    # on article contents\n",
    "    for ex in INTERVIEW_ENDING:\n",
    "        expression = REGULAR_EXPRESSION.format(ex)\n",
    "        matches = re.findall(expression, interview_text)\n",
    "        all_matches = all_matches + matches\n",
    "    all_matches = list(map(remove_special_character_without_spaces, all_matches))\n",
    "    counter = Counter(all_matches)\n",
    "    for elm in set(all_matches):\n",
    "        if len(elm) < THRESHOLD:\n",
    "            count_dict[elm] = counter[elm]\n",
    "    doc_exprsn[f'doc_{idx}'] = count_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc_0': {'joe': 16, '” then they say': 1, 'martin': 2},\n",
       " 'doc_1': {'jackson': 2,\n",
       "  'balaji s srinivasan': 1,\n",
       "  'srinivisan': 1,\n",
       "  'srinivasan': 12},\n",
       " 'doc_2': {'jon medved': 8, 'ekmh': 1},\n",
       " 'doc_3': {'laughter phil libin': 1,\n",
       "  'yeah nicole torres': 1,\n",
       "  'phil libin': 53,\n",
       "  'nicole torres': 18},\n",
       " 'doc_4': {'mackey': 19, 'reason': 1, ' mackey': 1, 'john mackey': 1},\n",
       " 'doc_5': {'tim cook': 30,\n",
       "  'charlie rose': 14,\n",
       "  'phil schiller': 1,\n",
       "  ' tim cook': 2,\n",
       "  'graham townsend': 1,\n",
       "  'angela ahrendts': 1,\n",
       "  'jony ive': 8}}"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_exprsn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc_0': {'martin': 24, '” then they say': 1},\n",
      " 'doc_1': {'jackson': 14, 'srinivisan': 1},\n",
      " 'doc_2': {'ekmh': 12},\n",
      " 'doc_3': {'nicole torres': 62, 'yeah nicole torres': 1},\n",
      " 'doc_4': {'reason': 22},\n",
      " 'doc_5': {'angela ahrendts': 2,\n",
      "           'charlie rose': 54,\n",
      "           'graham townsend': 5,\n",
      "           'jony ive': 13,\n",
      "           'phil schiller': 1}}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Interviewers and Interviewees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Interviewee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Interviewees names used in the article .......\n",
      "\n",
      "{'doc_0': 'joe',\n",
      " 'doc_1': 'srinivasan',\n",
      " 'doc_2': 'jon medved',\n",
      " 'doc_3': 'phil libin',\n",
      " 'doc_4': 'mackey',\n",
      " 'doc_5': 'tim cook'}\n"
     ]
    }
   ],
   "source": [
    "print('Predicting Interviewees names used in the article .......\\n')\n",
    "pprint(predict(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding potential interviewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Interviewers names used in the article .......\n",
      "\n",
      "{'doc_0': 'martin',\n",
      " 'doc_1': 'jackson',\n",
      " 'doc_2': 'ekmh',\n",
      " 'doc_3': 'nicole torres',\n",
      " 'doc_4': 'reason',\n",
      " 'doc_5': 'charlie rose'}\n"
     ]
    }
   ],
   "source": [
    "print('Predicting Interviewers names used in the article .......\\n')\n",
    "\n",
    "count = 0\n",
    "article_name_dict = {}\n",
    "for key, value in doc_exprsn.items():\n",
    "    temp_dict = {}\n",
    "    name_keys = list(value.keys())\n",
    "    data = remove_special_character_with_custom(str.lower(DATA[count].get('body')), custom=[':', ' '])\n",
    "    interviewer_names = doc.get(key)\n",
    "    potential_removers = [ j for k in interviewer_names.keys() for j in name_keys if k in j]\n",
    "    \n",
    "    for item in set(potential_removers):\n",
    "        name_keys.remove(item)\n",
    "    \n",
    "    for name_key in name_keys:\n",
    "        name_key = name_key.strip()\n",
    "        occurence = element_occurence(name_key, data)\n",
    "        if  occurence:\n",
    "            temp_dict[name_key] = occurence\n",
    "    article_name_dict[f'doc_{count}'] = temp_dict\n",
    "    count = count + 1\n",
    "pprint(predict(article_name_dict, softening=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
