{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library files import\n",
    "\n",
    "import cerberus\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building validation dict based on the structure below\n",
    "\n",
    "# {\n",
    "#  \"_id\": \"https://www.ekmhinnovators.com/ekmh-innovators-blog-beta/interview-ourcrowd-ceo-jon-medved-on-impact-investing-crowdfunding\",\n",
    "#  \"title\": \"Interview: OurCrowd CEO Jon Medved on Crowdfunding, Beyond ...\",\n",
    "#  \"body\": \"EKMH Innovators Interview Series An interview ...\",\n",
    "#  \"origin\": \"google custom search\",\n",
    "#  \"feedId\": 103,\n",
    "#  \"jobId\": \"37b3e04c-cf7d-4032-82ad-a2bd89dc90ac\",\n",
    "#  \"person\": {\n",
    "# \t \"id\": \"16\",\n",
    "# \t \"name\": \"Jon Medved\"\n",
    "#  }\n",
    "# }\n",
    "\n",
    "STRING_MANDATORY = {'type': 'string', 'empty': False, 'required': True}\n",
    "INT_MANDATORY = {'type': 'integer', 'empty': False, 'required': True}\n",
    "VALIDATION_SCHEMA = {\n",
    "    '_id': STRING_MANDATORY, \n",
    "    'title': STRING_MANDATORY, \n",
    "    'body': STRING_MANDATORY, \n",
    "    'origin': STRING_MANDATORY, \n",
    "    'feedId': INT_MANDATORY, \n",
    "    'jobId': STRING_MANDATORY, \n",
    "    'person': {\n",
    "        'type': 'dict', 'required': True, 'empty': False, \n",
    "            'schema': {\n",
    "                'id': STRING_MANDATORY,\n",
    "                'name': STRING_MANDATORY\n",
    "            }\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': {'type': 'string', 'empty': False, 'required': True},\n",
       " 'title': {'type': 'string', 'empty': False, 'required': True},\n",
       " 'body': {'type': 'string', 'empty': False, 'required': True},\n",
       " 'origin': {'type': 'string', 'empty': False, 'required': True},\n",
       " 'feedId': {'type': 'integer', 'empty': False, 'required': True},\n",
       " 'jobId': {'type': 'string', 'empty': False, 'required': True},\n",
       " 'person': {'type': 'dict',\n",
       "  'required': True,\n",
       "  'empty': False,\n",
       "  'schema': {'id': {'type': 'string', 'empty': False, 'required': True},\n",
       "   'name': {'type': 'string', 'empty': False, 'required': True}}}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# displaying validation dictionary\n",
    "\n",
    "VALIDATION_SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/0.json', './data/1.json', './data/2.json', './data/3.json', './data/4.json', './data/5.json']\n"
     ]
    }
   ],
   "source": [
    "# Path of the data files\n",
    "# Add more files to the project folder or change this path to read from any \n",
    "# other directory path\n",
    "DATA_FOLDER = os.path.join(os.path.curdir, 'data') # by default it takes the JSONs from project folder\n",
    "\n",
    "# only considering JSON files at the moment\n",
    "interview_files_list = [os.path.join(DATA_FOLDER, f) for f in os.listdir(DATA_FOLDER) \\\n",
    "                        if '.json' in os.path.splitext(f) ]\n",
    "\n",
    "\n",
    "print(interview_files_list)\n",
    "if len(interview_files_list) < 1:\n",
    "    proceed = False\n",
    "    error = \"No data file available\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Common Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "validate input data schema\n",
    "\"\"\"\n",
    "def validate_schema(entities, schema, filepath):\n",
    "    checker = cerberus.Validator()\n",
    "    checker.allow_unknown = True\n",
    "    if checker.validate(entities, schema):\n",
    "        print(f'Input schema validated for {filepath} !')\n",
    "    else:\n",
    "        errors = checker.errors\n",
    "        proceed = False\n",
    "        error = errors\n",
    "        raise ValueError(f'Format mismatch for the input {errors}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Remove special character\n",
    "\"\"\"\n",
    "def remove_special_character(text_to_process, discarded_elements=[], strip=False):\n",
    "    \n",
    "    new_string = str(text_to_process)\n",
    "    if strip:\n",
    "        new_string = new_string.strip()\n",
    "        \n",
    "    special_chars = string.printable[62:]\n",
    "    special_chars = [x for x in special_chars if x not in discarded_elements]\n",
    "    for char in text_to_process:\n",
    "        if char in special_chars:\n",
    "            new_string = new_string.replace(char, '')\n",
    "    return new_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Remove special character without spaces\n",
    "\"\"\"\n",
    "def remove_special_character_without_spaces(text_to_process, discarded_elements=[]):\n",
    "    return remove_special_character(text_to_process, [' '], strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Remove spaces\n",
    "\"\"\"\n",
    "def remove_spaces(text_to_process):\n",
    "    return text_to_process.replace(\" \", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Validate and read the JSON data\n",
    "file_path: path of the json file\n",
    "\"\"\"\n",
    "def read_and_validate_data(filepath):\n",
    "    with open(filepath) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        validate_schema(entities=data, schema=VALIDATION_SCHEMA,\\\n",
    "                        filepath=filepath)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Encode Text\n",
    "\"\"\"\n",
    "def encode_text(text_to_encode, char_to_index):\n",
    "    return np.array([char_to_index[c] for c in text_to_encode])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reverse the array\n",
    "\"\"\"\n",
    "def reverse_the_array(arr):\n",
    "    return arr[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom method and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_interviewee(name_doc_mapping={}):\n",
    "    predicted_interviewee = {}\n",
    "    for key, value in name_doc_mapping.items():\n",
    "        item_list = max(value.items(), key=lambda x: x[1])\n",
    "        labels = list()\n",
    "        for k, v in value.items():\n",
    "            if v == item_list[1]:\n",
    "                predicted_interviewee[key] = k\n",
    "    return predicted_interviewee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input schema validated for ./data/0.json !\n",
      "Input schema validated for ./data/1.json !\n",
      "Input schema validated for ./data/2.json !\n",
      "Input schema validated for ./data/3.json !\n",
      "Input schema validated for ./data/4.json !\n",
      "Input schema validated for ./data/5.json !\n"
     ]
    }
   ],
   "source": [
    "# Reading in the data input files while also \n",
    "# validating them\n",
    "data = [read_and_validate_data(file) for file in interview_files_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_interview_encoding(interview_contents):\n",
    "#     total_chars = sorted(set(interview_contents))\n",
    "#     char_to_index = {char:index for index, char in enumerate(total_chars)}\n",
    "#     index_to_char = np.array(total_chars)\n",
    "#     encoded_text = np.array([char_to_index[c] for c in interview_contents])\n",
    "#     return char_to_index, index_to_char, encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predict interviewer and interviewee name\n",
    "# interview_content = str.lower(data[0]['body'])\n",
    "# interview_content_no_special_chars = remove_special_character(interview_content, [':'])\n",
    "# char_to_index, index_to_char, encoded_text = \\\n",
    "#     create_interview_encoding(interview_content_no_special_chars)\n",
    "\n",
    "\n",
    "# interviewee = data[0]['person']['name']\n",
    "# sub_names = interviewee.split()\n",
    "# sub_names.append(interviewee)\n",
    "# names = [encode_text(remove_special_character(x.lower())\\\n",
    "#                      , char_to_index) for x in sub_names]\n",
    "\n",
    "# reverse_encoded_text=reverse_the_array(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interview_content = str.lower(data[2]['body'])\n",
    "# interview_content_no_special_chars = remove_special_character(interview_content, [':', '.', '?', ';', '\"'])\n",
    "# print(interview_content_no_special_chars[:100])\n",
    "# reverse_encoded_text=reverse_the_array(interview_content_no_special_chars)\n",
    "# print(reverse_encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse_encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expression_one = '(?<=\\:)(.*?)(?=\\{})'\n",
    "\n",
    "# all_matches = []\n",
    "# count_dict = {}\n",
    "\n",
    "# exprsns = ['.', '?', \";\"]\n",
    "# for ex in exprsns:\n",
    "#     expression = expression_one.format(ex)\n",
    "#     matches = re.findall(expression, reverse_encoded_text)\n",
    "#     all_matches = all_matches + matches\n",
    "\n",
    "# counter = Counter(all_matches)\n",
    "\n",
    "# for elm in set(all_matches):\n",
    "#     if len(elm) < 25:\n",
    "#         count_dict[elm] = counter[elm]\n",
    "# #         print(type(counter[elm]))\n",
    "        \n",
    "    \n",
    "# # expression_two = r\"(?<=\\:)(.*?)(?=\\?)\"\n",
    "# # matches_one = re.findall(expression_one, reverse_encoded_text)\n",
    "# # matches_two = re.findall(expression_two, reverse_encoded_text)\n",
    "\n",
    "# # matches = matches_one + matches_two\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# d = Counter(matches_one)\n",
    "# for elem in set(matches):\n",
    "#     if len(elem) < 15:\n",
    "#         print(elem, d[elem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(matches_two) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = reverse_encoded_text\n",
    "# start = s.find(':')\n",
    "# end = s.find('.')\n",
    "# substring = s[start:end]\n",
    "# print(substring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buckets = []\n",
    "\n",
    "# def find_pattern(text_data, names=[]):\n",
    "#     if not names:\n",
    "#         raise Exception(f'Pattern could not be found, names = {names}')\n",
    "#     text_split=text_data.split(':')\n",
    "#     name_dict = {}\n",
    "    \n",
    "#     for name in names:\n",
    "#         for text in text_split:\n",
    "#             name_count = name_dict.get(name, 0)\n",
    "#             name_dict[name] = text.count(name) + name_count\n",
    "#             if text.find(name) == 0:\n",
    "#                 print(name, text[:10], text.find(name))\n",
    "\n",
    "        \n",
    "    \n",
    "# #     print(name_dict)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc = {}\n",
    "expression_one = '(?<=\\{})(.*?)(?=\\:)'\n",
    "exprsns = ['.', '?', \";\"]\n",
    "\n",
    "doc_exprsn = {}\n",
    "\n",
    "\n",
    "for idx, entity in enumerate(data):\n",
    "    count_dict = {}\n",
    "    all_matches = []\n",
    "    interview_text = str.lower(entity.get('body'))\n",
    "    interview_without_spl_chr = interview_text\n",
    "    name_dict = {}\n",
    "    interviewee = entity.get('person')['name']\n",
    "    sub_names = interviewee.split()\n",
    "    sub_names.append(interviewee)\n",
    "    names = [x.lower() for x in sub_names]\n",
    "    for name in names:\n",
    "        name_dict[name] = len([pos.start() for pos in re.finditer(f'{name}:', interview_without_spl_chr)])\n",
    "    doc[f'doc_{idx}'] = name_dict\n",
    "    for ex in exprsns:\n",
    "        expression = expression_one.format(ex)\n",
    "        matches = re.findall(expression, interview_without_spl_chr)\n",
    "        all_matches = all_matches + matches\n",
    "    all_matches = list(map(remove_special_character_without_spaces, all_matches))\n",
    "    counter = Counter(all_matches)\n",
    "    for elm in set(all_matches):\n",
    "        if len(elm) < 25:\n",
    "            count_dict[elm] = counter[elm]\n",
    "    doc_exprsn[f'doc_{idx}'] = count_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc_0': 'joe', 'doc_1': 'srinivasan', 'doc_2': 'jon medved', 'doc_3': 'phil libin', 'doc_4': 'mackey', 'doc_5': 'tim cook'}\n"
     ]
    }
   ],
   "source": [
    "print(predict_interviewee(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc_0': {'joe': 16,\n",
       "  'martin': 2,\n",
       "  'i appreciate it martin': 1,\n",
       "  '” then they say': 1},\n",
       " 'doc_1': {'srinivasan': 12,\n",
       "  'srinivisan': 1,\n",
       "  'jackson': 2,\n",
       "  'balaji s srinivasan': 1},\n",
       " 'doc_2': {'jon medved': 8, 'ekmh': 1},\n",
       " 'doc_3': {'phil libin': 53,\n",
       "  'that too nicole torres': 1,\n",
       "  'yeah nicole torres': 1,\n",
       "  'laughter phil libin': 1,\n",
       "  'nicole torres': 18},\n",
       " 'doc_4': {' mackey': 1, 'john mackey': 1, 'mackey': 19, 'reason': 1},\n",
       " 'doc_5': {'tim cook': 30,\n",
       "  'angela ahrendts': 1,\n",
       "  'jony ive': 8,\n",
       "  ' tim cook': 2,\n",
       "  'charlie rose': 14,\n",
       "  'graham townsend': 1,\n",
       "  'phil schiller': 1}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_exprsn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_interviewers(doc_exprsn):\n",
    "    potential_interviewers = {}\n",
    "    for key, value in doc_exprsn.items():\n",
    "        print(key, value)\n",
    "        \n",
    "        \n",
    "#         item_list = max(value.items(), key=lambda x: x[1])\n",
    "#         labels = list()\n",
    "#         for k, v in value.items():\n",
    "#             if v == item_list[1]:\n",
    "#                 predicted_interviewee[key] = k\n",
    "#     return predicted_interviewee "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_0 {'joe': 16, 'martin': 2, 'i appreciate it martin': 1, '” then they say': 1}\n",
      "doc_1 {'srinivasan': 12, 'srinivisan': 1, 'jackson': 2, 'balaji s srinivasan': 1}\n",
      "doc_2 {'jon medved': 8, 'ekmh': 1}\n",
      "doc_3 {'phil libin': 53, 'that too nicole torres': 1, 'yeah nicole torres': 1, 'laughter phil libin': 1, 'nicole torres': 18}\n",
      "doc_4 {' mackey': 1, 'john mackey': 1, 'mackey': 19, 'reason': 1}\n",
      "doc_5 {'tim cook': 30, 'angela ahrendts': 1, 'jony ive': 8, ' tim cook': 2, 'charlie rose': 14, 'graham townsend': 1, 'phil schiller': 1}\n"
     ]
    }
   ],
   "source": [
    "predict_interviewers(doc_exprsn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "    # print('Keys with maximum Value in Dictionary : ', listOfKeys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicted_interviewee' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-c68ea4672383>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredicted_interviewee\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'predicted_interviewee' is not defined"
     ]
    }
   ],
   "source": [
    "predicted_interviewee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
